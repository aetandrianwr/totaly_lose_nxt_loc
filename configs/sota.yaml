# SOTA configuration with proven techniques from top research
model:
  name: "sota"
  params:
    num_locations: 1187
    num_users: 46
    embedding_dim: 64
    num_heads: 4
    num_layers: 2
    num_experts: 3
    dropout: 0.2

training:
  batch_size: 48  # Smaller batch for better generalization
  num_epochs: 300
  learning_rate: 0.0015
  weight_decay: 0.00015
  gradient_clip: 1.0
  early_stopping_patience: 50
  
  # Stochastic Weight Averaging (SWA) - NYU/Cornell
  use_swa: true
  swa_start: 20
  swa_lr: 0.0005
  
  # EMA
  use_ema: true
  ema_decay: 0.998
  
  # Learning rate schedule
  lr_scheduler: "cosine_warmup"
  warmup_epochs: 15
  
  # Data augmentation
  use_temporal_jitter: true
  jitter_prob: 0.15
  
  # Label smoothing for better calibration
  label_smoothing: 0.08

data:
  train_path: "data/geolife/geolife_transformer_7_train.pk"
  val_path: "data/geolife/geolife_transformer_7_validation.pk"
  test_path: "data/geolife/geolife_transformer_7_test.pk"
  max_seq_len: 60
  num_workers: 4

optimizer:
  name: "adamw"
  betas: [0.9, 0.999]
  eps: 1.0e-8

loss:
  label_smoothing: 0.08  # Better calibration
  
system:
  seed: 42
  device: "cuda"
  checkpoint_dir: "checkpoints"
  log_dir: "logs"
  save_freq: 5
  
logging:
  use_tensorboard: false
  print_freq: 50
